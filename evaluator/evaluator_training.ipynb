{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470355ef-d489-4091-8ba3-382a20d27d53",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1a94-1016-4113-8c81-9aa15aaf6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std libraries\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import html\n",
    "import time\n",
    "import io\n",
    "import hashlib\n",
    "import tarfile\n",
    "import json\n",
    "\n",
    "# 3rd party libraries\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LinearLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fe281-f594-4993-9b08-145b3cde9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe string constants\n",
    "RATER_PARTICIPANT_ID = \"raterParticipantId\"\n",
    "HELPFUL_VALUE_TSV = \"HELPFUL\"\n",
    "NOT_HELPFUL_VALUE_TSV = \"NOT_HELPFUL\"\n",
    "SOMEWHAT_HELPFUL_VALUE_TSV = \"SOMEWHAT_HELPFUL\"\n",
    "HELPFULNESS_LEVEL_KEY = \"helpfulnessLevel\"\n",
    "HELPFUL_NUM_KEY = \"helpfulNum\"\n",
    "CORE_NOTE_INTERCEPT = \"coreNoteIntercept\"\n",
    "EXPANSION_NOTE_INTERCEPT = \"expansionNoteIntercept\"\n",
    "EXPANSION_PLUS_NOTE_INTERCEPT = \"expansionPlusNoteIntercept\"\n",
    "CORE_NOTE_FACTOR = \"coreNoteFactor1\"\n",
    "EXPANSION_NOTE_FACTOR = \"expansionNoteFactor1\"\n",
    "EXPANSION_PLUS_NOTE_FACTOR = \"expansionPlusNoteFactor1\"\n",
    "EXPANSION_RATER_INTERCEPT = \"expansionRaterIntercept\"\n",
    "EXPANSION_RATER_FACTOR = \"expansionRaterFactor1\"\n",
    "NOTE_ID = \"note_id\"\n",
    "TWEET_ID = \"tweet_id\"\n",
    "NOTE_TEXT = \"note_text\"\n",
    "TWEET_TEXT = \"tweet_text\"\n",
    "TWEET_SHORTEN_URLS = \"tweet_shorten_urls\"\n",
    "TWEET_EXPANDED_URLS = \"tweet_expanded_urls\"\n",
    "CLASSIFICATION = \"classification\"\n",
    "CURRENT_LABEL = \"currentStatus\"\n",
    "INTERCEPT = \"intercept\"\n",
    "FACTOR = \"factor\"\n",
    "NOTE_TEXT_UNESCAPED = \"note_text_unescaped\"\n",
    "TWEET_TEXT_UNESCAPED = \"tweet_text_unescaped\"\n",
    "NOTE_TEXT_FINAL = \"note_text_final\"\n",
    "TWEET_TEXT_FINAL = \"tweet_text_final\"\n",
    "NOTE_LANG = \"note_lang\"\n",
    "NOTE_LANG_CONFIDENCE = \"note_lang_confidence\"\n",
    "NOTE_LANG_INFERRED = \"note_lang_inferred\"\n",
    "TWEET_LANG = \"tweet_lang\"\n",
    "TWEET_LANG_CONFIDENCE = \"tweet_lang_confidence\"\n",
    "TWEET_LANG_INFERRED = \"tweet_lang_inferred\"\n",
    "RATING_WEIGHT = \"rating_weight\"\n",
    "TOTAL_SIGNAL = \"total_signal\"\n",
    "CAMEL_NOTE_ID = \"noteId\"\n",
    "PREDICTED_HELPFULNESS = \"predicted_helpfulness\"\n",
    "MISINFORMED_OR_POTENTIALLY_MISLEADING = \"MISINFORMED_OR_POTENTIALLY_MISLEADING\"\n",
    "CURRENTLY_RATED_NOT_HELPFUL = \"CURRENTLY_RATED_NOT_HELPFUL\"\n",
    "NOT_MISLEADING = \"NOT_MISLEADING\"\n",
    "CRNH = \"crnh\"\n",
    "LARGE_FACTOR = \"large_factor\"\n",
    "RELEVANCE = \"relevance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a98a13-2e72-4b71-a77e-132ca9f81895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target labels\n",
    "TAG_LABEL_COLS = [\n",
    "  \"notHelpfulSourcesMissingOrUnreliable\",\n",
    "  \"notHelpfulArgumentativeOrBiased\",\n",
    "  \"notHelpfulSpamHarassmentOrAbuse\",\n",
    "  \"notHelpfulIrrelevantSources\",\n",
    "  \"notHelpfulOpinionSpeculation\",\n",
    "  \"notHelpfulNoteNotNeeded\",\n",
    "]\n",
    "LABEL_COLS = [\n",
    "  RELEVANCE,\n",
    "  CLASSIFICATION,\n",
    "  CRNH,\n",
    "  LARGE_FACTOR\n",
    "] + TAG_LABEL_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125ef60-0200-4518-a886-169436e6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional constants\n",
    "CUDA = \"cuda\"\n",
    "CPU = \"cpu\"\n",
    "ROOT = os.path.expanduser(\"~/workspace\")\n",
    "HF_ROOT = os.path.join(ROOT, \"huggingface\")\n",
    "MODEL_ROOT = os.path.join(HF_ROOT, \"models\")\n",
    "MODEL_DIR = \"model\"\n",
    "TOKENIZER_DIR = \"tokenizer\"\n",
    "LANGUAGE_DETECTION_MODEL = \"xlm-roberta-base-language-detection\"\n",
    "DISTILROBERTA_BASE_MODEL = \"distilroberta-base\"\n",
    "DATA_ROOT = os.path.join(ROOT, \"datasets/evaluator\")\n",
    "EXPANSION_GLOBAL_BIAS = 0.17178\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5cf4b-41c6-4836-abb2-d61dfcdad893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper for monitoring GPU memory usage\n",
    "def get_gpu_stats():\n",
    "  tmp = !nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv\n",
    "  return pd.DataFrame([row.split(\",\") for row in tmp[1:]], columns=tmp[0].split(\",\"))\n",
    "\n",
    "get_gpu_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67a414-4ef1-4031-8f79-b548aadb2a63",
   "metadata": {},
   "source": [
    "# Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211de21-1b8e-4841-aacd-bb7327f1c179",
   "metadata": {},
   "source": [
    "## Load and Prune Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac79aba-3d87-4d29-b1d7-18eedd81b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoring inputs\n",
    "ratings = pd.read_parquet(os.path.join(DATA_ROOT, \"ratings.parquet\"))\n",
    "notes = pd.read_parquet(os.path.join(DATA_ROOT, \"notes.parquet\"))\n",
    "nsh = pd.read_parquet(os.path.join(DATA_ROOT, \"note_status_history.parquet\"))                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dfb4c-054b-460e-8730-f9f0602b0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoring outputs\n",
    "scoredRaters = pd.read_parquet(os.path.join(DATA_ROOT, \"scored_raters.parquet\"))\n",
    "scoredNotes = pd.read_parquet(os.path.join(DATA_ROOT, \"scored_notes.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460a34c-3133-457f-b0f9-464285455c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load posts\n",
    "posts = pd.read_parquet(os.path.join(DATA_ROOT, \"posts.parquet\"))\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf302ab5-c911-435d-b225-04b49d3db47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize types\n",
    "ratings[RATER_PARTICIPANT_ID] = ratings[RATER_PARTICIPANT_ID].astype(np.int64)\n",
    "scoredRaters[RATER_PARTICIPANT_ID] = scoredRaters[RATER_PARTICIPANT_ID].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5daa387-142c-4634-911b-a37d7a2a075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize note_id column name\n",
    "ratings = ratings.rename(columns={CAMEL_NOTE_ID: NOTE_ID})\n",
    "notes = notes.rename(columns={CAMEL_NOTE_ID: NOTE_ID})\n",
    "nsh = nsh.rename(columns={CAMEL_NOTE_ID: NOTE_ID})\n",
    "scoredNotes = scoredNotes.rename(columns={CAMEL_NOTE_ID: NOTE_ID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d5337-466c-4b01-b099-f2d57f89e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune columns\n",
    "ratings = ratings[[NOTE_ID, RATER_PARTICIPANT_ID, HELPFULNESS_LEVEL_KEY] + TAG_LABEL_COLS]\n",
    "posts = posts[[NOTE_ID, TWEET_ID, NOTE_TEXT, TWEET_TEXT, TWEET_SHORTEN_URLS, TWEET_EXPANDED_URLS]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ddf21-80aa-435f-8c80-a66808d051c2",
   "metadata": {},
   "source": [
    "## Compute Weighted Tag Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e2bb7-8dcb-4322-8035-15f95b23ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment ratings with standardized helpfulness level and scoring results for notes and raters\n",
    "def add_level_and_scoring_results(ratings, scoredRaters, scoredNotes):\n",
    "  # Select columns and set helpfulNum\n",
    "  print(f\"Original ratings: {len(ratings)}\")\n",
    "  ratings[HELPFUL_NUM_KEY] = np.nan\n",
    "  ratings.loc[ratings[HELPFULNESS_LEVEL_KEY] == HELPFUL_VALUE_TSV, HELPFUL_NUM_KEY] = 1.0\n",
    "  ratings.loc[ratings[HELPFULNESS_LEVEL_KEY] == SOMEWHAT_HELPFUL_VALUE_TSV, HELPFUL_NUM_KEY] = 0.5\n",
    "  ratings.loc[ratings[HELPFULNESS_LEVEL_KEY] == NOT_HELPFUL_VALUE_TSV, HELPFUL_NUM_KEY] = 0.0\n",
    "  ratings = ratings[ratings[HELPFUL_NUM_KEY].notna()].drop(columns=HELPFULNESS_LEVEL_KEY)\n",
    "  print(f\"Ratings with helpfulNum: {len(ratings)}\")\n",
    "  # Augment with scoring results\n",
    "  ratings = ratings.merge(scoredRaters[[RATER_PARTICIPANT_ID, EXPANSION_RATER_FACTOR, EXPANSION_RATER_INTERCEPT]].dropna())\n",
    "  ratings = ratings.merge(scoredNotes[[NOTE_ID, EXPANSION_NOTE_FACTOR, EXPANSION_NOTE_INTERCEPT]].dropna())\n",
    "  print(f\"Ratings with scoring results: {len(ratings)}\")\n",
    "  assert ratings.isna().sum().sum() == 0\n",
    "  return ratings\n",
    "\n",
    "ratings = add_level_and_scoring_results(ratings, scoredRaters, scoredNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d489582-86de-4bfe-a7dc-c411199a54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a prediction of how we expect a rater to rate a note based on the learned viewpoint\n",
    "# representation for the note and rater, as well as bias terms.\n",
    "# Notice that we add the mean of the note intercept to shift predictions appropriately without\n",
    "# actually incorporating quality signal specific to the note.\n",
    "def add_prediction(ratings):\n",
    "  ratings[PREDICTED_HELPFULNESS] = (\n",
    "    ratings[EXPANSION_RATER_FACTOR] * ratings[EXPANSION_NOTE_FACTOR]\n",
    "    + ratings[EXPANSION_RATER_INTERCEPT]\n",
    "    + ratings[EXPANSION_NOTE_INTERCEPT].mean()\n",
    "    + EXPANSION_GLOBAL_BIAS\n",
    "  )\n",
    "  return ratings\n",
    "\n",
    "ratings = add_prediction(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57e98d-b859-49ec-baf7-a25ad0946044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile learned representation\n",
    "def profile_params(scoredRaters, scoredNotes):\n",
    "  fig, ax = plt.subplots(1, 4)\n",
    "  fig.set_figwidth(30)\n",
    "  fig.set_figheight(5)\n",
    "  scoredRaters[EXPANSION_RATER_FACTOR].plot.hist(bins=50, ax=ax[0], title=\"Rater Factors\")\n",
    "  scoredRaters[EXPANSION_RATER_INTERCEPT].plot.hist(bins=50, ax=ax[1], title=\"Rater Intercepts\")\n",
    "  scoredNotes[EXPANSION_NOTE_FACTOR].plot.hist(bins=50, ax=ax[2], title=\"Note Factors\")\n",
    "  scoredNotes[EXPANSION_NOTE_INTERCEPT].plot.hist(bins=50, ax=ax[3], title=\"Note Intercepts\")\n",
    "\n",
    "profile_params(scoredRaters, scoredNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c5529-c93a-4b02-8e28-950f5b4a58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of predictions\n",
    "def plot_predictions(predRatings):\n",
    "  fig, ax = plt.subplots(1, 4, sharex=True)\n",
    "  fig.set_figwidth(30)\n",
    "  fig.set_figheight(5)\n",
    "  predRatings[PREDICTED_HELPFULNESS].plot.hist(bins=50, ax=ax[0])\n",
    "  ax[0].set_title(\"All\")\n",
    "  predRatings[predRatings[HELPFUL_NUM_KEY] == 1.0][PREDICTED_HELPFULNESS].plot.hist(bins=50, ax=ax[1])\n",
    "  ax[1].set_title(\"Helpful\")\n",
    "  predRatings[predRatings[HELPFUL_NUM_KEY] == 0.5][PREDICTED_HELPFULNESS].plot.hist(bins=50, ax=ax[2])\n",
    "  ax[2].set_title(\"Somewhat Helpful\")\n",
    "  predRatings[predRatings[HELPFUL_NUM_KEY] == 0.0][PREDICTED_HELPFULNESS].plot.hist(bins=50, ax=ax[3])\n",
    "  ax[3].set_title(\"Not Helpful\")\n",
    "\n",
    "plot_predictions(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b35867-a005-47a8-871c-4261e4aea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions passed through sigmoid for weighting\n",
    "def plot_pred_sigmoid(predRatings):\n",
    "  fig, ax = plt.subplots(1, 4, sharex=True)\n",
    "  fig.set_figwidth(30)\n",
    "  fig.set_figheight(5)\n",
    "  std = predRatings[PREDICTED_HELPFULNESS].std()\n",
    "  for i, multiplier in enumerate([1, 1.5, 2, 3]):\n",
    "    factor = multiplier / std\n",
    "    # Center the predicted helpfulness around .5 since the MF treats Helpful as 1 and Not Helpful as 0.\n",
    "    # Scale by the std deviation and a multiplicative factor to determine how strongly to weight ratings.\n",
    "    # Apply sigmoid.\n",
    "    ((1 + np.exp(-1 * factor * (predRatings[PREDICTED_HELPFULNESS] - .5))) ** -1).plot.hist(bins=50, title=f\"factor={multiplier}\", ax=ax[i])\n",
    "\n",
    "plot_pred_sigmoid(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae11b11-cb9f-4f18-b2bb-3e504051b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine rating weights with multiplier=1\n",
    "def get_weighted_tag_ratios(ratings, multiplier):\n",
    "  ratings = ratings.copy()\n",
    "  factor = multiplier / ratings[PREDICTED_HELPFULNESS].std()\n",
    "  ratings[RATING_WEIGHT] = ((1 + np.exp(-1 * factor * (ratings[PREDICTED_HELPFULNESS] - .5))) ** -1)\n",
    "  for col in TAG_LABEL_COLS:\n",
    "    ratings[col] = ratings[col] * ratings[RATING_WEIGHT]\n",
    "  scores = ratings[[NOTE_ID, RATING_WEIGHT] + TAG_LABEL_COLS].groupby(NOTE_ID).sum().reset_index(drop=False).rename(\n",
    "    columns={RATING_WEIGHT: TOTAL_SIGNAL})\n",
    "  for col in TAG_LABEL_COLS:\n",
    "    scores[f\"{col}_ratio\"] = scores[col] / scores[TOTAL_SIGNAL]\n",
    "  print(f\"Total notes: {len(scores)}\")\n",
    "  return scores\n",
    "\n",
    "weightedTagRatios = get_weighted_tag_ratios(ratings, 1)\n",
    "weightedTagRatios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36662356-59be-49d3-b690-e0d369f9bad9",
   "metadata": {},
   "source": [
    "## Assemble Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df4dc1-0088-470f-80ed-e9fc21af3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce note factors\n",
    "def get_note_factor(scoredNotes):\n",
    "\n",
    "  def _get_factor(core, expansion, expansionPlus):\n",
    "    if not pd.isna(core):\n",
    "      return core\n",
    "    if not pd.isna(expansion):\n",
    "      return expansion\n",
    "    if not pd.isna(expansionPlus):\n",
    "      return expansionPlus\n",
    "    return np.nan\n",
    "  scoredNotes = scoredNotes[[NOTE_ID, CORE_NOTE_FACTOR, EXPANSION_NOTE_FACTOR, EXPANSION_PLUS_NOTE_FACTOR]].copy()\n",
    "  scoredNotes[FACTOR] = [_get_factor(core, expansion, expansionPlus) for (core, expansion, expansionPlus) in (\n",
    "    scoredNotes[[CORE_NOTE_FACTOR, EXPANSION_NOTE_FACTOR, EXPANSION_PLUS_NOTE_FACTOR]].values\n",
    "  )]\n",
    "  return scoredNotes[[NOTE_ID, FACTOR]].rename(columns={NOTE_ID: NOTE_ID})\n",
    "\n",
    "noteFactors = get_note_factor(scoredNotes)\n",
    "noteFactors.merge(scoredNotes[[NOTE_ID, CORE_NOTE_FACTOR, EXPANSION_NOTE_FACTOR, EXPANSION_PLUS_NOTE_FACTOR]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae575ba1-7a25-412b-bf01-783b6a436837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce note intercepts\n",
    "def get_note_intercept(scoredNotes):\n",
    "\n",
    "  def _get_intercept(core, expansion, expansionPlus):\n",
    "    if not pd.isna(core):\n",
    "      return core\n",
    "    if not pd.isna(expansion):\n",
    "      return expansion\n",
    "    if not pd.isna(expansionPlus):\n",
    "      return expansionPlus\n",
    "    return np.nan\n",
    "  scoredNotes = scoredNotes[[NOTE_ID, CORE_NOTE_INTERCEPT, EXPANSION_NOTE_INTERCEPT, EXPANSION_PLUS_NOTE_INTERCEPT]].copy()\n",
    "  scoredNotes[INTERCEPT] = [_get_intercept(core, expansion, expansionPlus) for (core, expansion, expansionPlus) in (\n",
    "    scoredNotes[[CORE_NOTE_INTERCEPT, EXPANSION_NOTE_INTERCEPT, EXPANSION_PLUS_NOTE_INTERCEPT]].values\n",
    "  )]\n",
    "  return scoredNotes[[NOTE_ID, INTERCEPT]].rename(columns={NOTE_ID: NOTE_ID})\n",
    "\n",
    "noteIntercepts = get_note_intercept(scoredNotes)\n",
    "noteIntercepts.merge(scoredNotes[[NOTE_ID, CORE_NOTE_INTERCEPT, EXPANSION_NOTE_INTERCEPT, EXPANSION_PLUS_NOTE_INTERCEPT]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f9db9-8677-4b6b-8391-677419bc019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all signals for a final export dataset\n",
    "def prepare_dataset(posts, noteFactors, noteIntercepts, nsh, notes, weightedTagRatios):\n",
    "  # Extract classifications and final note status\n",
    "  finalStatus = nsh[[NOTE_ID, CURRENT_LABEL]]\n",
    "  classifications = notes[[NOTE_ID, CLASSIFICATION]]\n",
    "  # Compose and return dataset\n",
    "  print(len(posts))\n",
    "  dataset = posts.merge(\n",
    "    noteFactors, on=NOTE_ID).merge(\n",
    "    noteIntercepts, on=NOTE_ID).merge(\n",
    "    finalStatus, on=NOTE_ID).merge(\n",
    "    classifications, on=NOTE_ID).merge(\n",
    "    weightedTagRatios, on=NOTE_ID)\n",
    "  print(len(dataset))\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581ab3b-63a7-46ec-bdf9-56c4fec54273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(posts, noteFactors, noteIntercepts, nsh, notes, weightedTagRatios)\n",
    "print(len(dataset))\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2abbc0-c4d6-4638-9d79-f9285f0eb095",
   "metadata": {},
   "source": [
    "# Prepare Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a80ca-2193-4113-8a00-03cc1fd9a970",
   "metadata": {},
   "source": [
    "## Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf42570-557f-4df4-91bf-db49402b8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that text is always present\n",
    "dataset[[NOTE_TEXT, TWEET_TEXT]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18335e-2444-453f-8944-3a07c3ff2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that text is always present\n",
    "(dataset[[NOTE_TEXT, NOTE_TEXT]] == \"\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0b188-93ac-4229-b4ca-d416419fc04f",
   "metadata": {},
   "source": [
    "## Unescape Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f97571-53c9-42f2-9583-265d6e345673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unescape helper\n",
    "def unescape(text):\n",
    "  return html.unescape(html.unescape(text)) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253622a-eb93-4fa4-b00a-dba527bcfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unescape notes and tweets\n",
    "dataset[NOTE_TEXT_UNESCAPED] = [unescape(text) for text in dataset[NOTE_TEXT]]\n",
    "dataset[TWEET_TEXT_UNESCAPED] = [unescape(text) for text in dataset[TWEET_TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa838103-2993-42e3-9899-1c5fa26fcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show text sample\n",
    "for tmp in dataset[NOTE_TEXT_UNESCAPED].sample(10, random_state=SEED):\n",
    "  print(tmp)\n",
    "  print(\"------------------\"*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c93fb-a81f-4d1c-8671-6552ad73e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show text sample\n",
    "for tmp in dataset[TWEET_TEXT_UNESCAPED].sample(10, random_state=SEED):\n",
    "  print(tmp)\n",
    "  print(\"------------------\"*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f069e-fbe5-4a9a-ac66-56007cb882a5",
   "metadata": {},
   "source": [
    "## Prepare URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23219942-3822-4d13-9193-a58e49c2e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to replace URLs with full text versions\n",
    "def replace_urls(text, shortUrls, fullUrls, maxLength=150):\n",
    "  if shortUrls is not None:\n",
    "    # Validate mapping and replace known links\n",
    "    assert len(shortUrls) == len(fullUrls)\n",
    "    for short, full in zip(shortUrls, fullUrls):\n",
    "      text = text.replace(short, full[:maxLength])\n",
    "  # Remove any remaining shortlinks\n",
    "  return re.sub(\"https://t.co/\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f5474-6c4f-4dd8-a2ce-972edf98058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new dataset column with patched text\n",
    "dataset[TWEET_TEXT_FINAL] = [\n",
    "  replace_urls(text, shortUrls, fullUrls)\n",
    "  for (text, shortUrls, fullUrls)\n",
    "  in dataset[[TWEET_TEXT_UNESCAPED, TWEET_SHORTEN_URLS, TWEET_EXPANDED_URLS]].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5c787-ca06-4348-8955-b05e41930257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of patched values\n",
    "for tmp in dataset[[NOTE_ID, TWEET_TEXT_FINAL]].sample(10).values:\n",
    "  print(tmp)\n",
    "  print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce4354-8b0f-45b9-ba61-85f6db5cfe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to truncated URLs in note text\n",
    "def truncate_urls(noteText, maxLength=150):\n",
    "  assert maxLength >= 0\n",
    "  urlPattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "  def truncate_match(match):\n",
    "    url = match.group(0)\n",
    "    return url[:maxLength] if len(url) > maxLength else url\n",
    "  return re.sub(urlPattern, truncate_match, noteText)\n",
    "\n",
    "print(truncate_urls(\"This note has no url\"))\n",
    "print(truncate_urls(\"This note has 1 url http://www.foobar.com/test/path and then more text\", maxLength=15))\n",
    "print(truncate_urls(\"This note has 1 url http://foobar.com/test/path and then more text\", maxLength=15))\n",
    "print(truncate_urls(\"This note has 1 url https://www.foobar.com/test/path and then more text\", maxLength=15))\n",
    "print(truncate_urls(\"This note has 1 url https://foobar.com/test/path and then more text\", maxLength=15))\n",
    "print(truncate_urls(\"This note has 2 url https://foobar.com/test/path and https://foobarbaz.com/test/path then more text\", maxLength=15))\n",
    "print(truncate_urls(\"This note has 2 url https://foobar.com/test/path and https://foobarbaz.com/test/path then more text\", maxLength=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd2b32-c0cc-4714-8d16-dc08e2971c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply truncation to note text\n",
    "dataset[NOTE_TEXT_FINAL] = [truncate_urls(text) for text in dataset[NOTE_TEXT_UNESCAPED]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754b1c1-04b1-471f-a94f-a6a1ec120ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of patched values\n",
    "for tmp in dataset[NOTE_TEXT_FINAL].sample(10):\n",
    "  print(tmp)\n",
    "  print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa3305-dc58-4740-a576-657c4b7bb271",
   "metadata": {},
   "source": [
    "## Inspect Final Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bad7b-aa64-4b77-9f29-e003dab14b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that text is always present\n",
    "dataset[[NOTE_TEXT_FINAL, TWEET_TEXT_FINAL]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25c616-be20-4a8e-8072-6833ef7533e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that text is always present\n",
    "(dataset[[NOTE_TEXT_FINAL, TWEET_TEXT_FINAL]] == \"\").sum()  # non-zero expected because some tweets only contain a media short link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d9020-75e3-4160-aa15-09d36d525474",
   "metadata": {},
   "source": [
    "# Detect Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0c383-80d8-4efa-8e91-538c4bbcbe29",
   "metadata": {},
   "source": [
    "## Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70554504-dc1e-45ea-8a70-dc6b75b3a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "langDetectionModel = nn.DataParallel(AutoModelForSequenceClassification.from_pretrained(os.path.join(MODEL_ROOT, LANGUAGE_DETECTION_MODEL, MODEL_DIR)).to(CUDA))\n",
    "langDetectionTokenizer = AutoTokenizer.from_pretrained(os.path.join(MODEL_ROOT, LANGUAGE_DETECTION_MODEL, TOKENIZER_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed3b65-611c-49ab-82c5-4daf181e8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpu_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924ec47-61b1-4762-8ad2-16a94f9cdd45",
   "metadata": {},
   "source": [
    "## Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a99ea5-bfe7-48e5-b582-0f4c11b29fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model size\n",
    "print(f\"{sum(tmp.numel() for tmp in langDetectionModel.parameters())//(2**20)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24f9ef-fd40-429a-bec3-cf091cfa39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to classify a chunk\n",
    "def classify_chunk(texts, model, tokenizer):\n",
    "  inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "  preds = torch.softmax(logits, dim=-1)\n",
    "  vals, idxs = torch.max(preds, dim=1)\n",
    "  # Map raw predictions to languages\n",
    "  id2lang = model.module.config.id2label\n",
    "  return [(id2lang[k.item()], v.item()) for k, v in zip(idxs, vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898be17-53c0-4225-b36e-b02e2fa23422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to classify larger lists\n",
    "def classify_texts(texts, model, tokenizer, batchSize=1024):\n",
    "  start = 0\n",
    "  numBatches = int(np.ceil(len(texts) / batchSize))\n",
    "  results = []\n",
    "  progressBar = tqdm(range(numBatches))\n",
    "  while start < len(texts):\n",
    "    end = start + batchSize\n",
    "    results.extend(classify_chunk(texts[start:end], model, tokenizer))\n",
    "    progressBar.update(1)\n",
    "    start = end\n",
    "  return list(zip(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422c0b4-106e-4a79-9399-84e3275695aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute note languages\n",
    "noteLangs, noteConfidence = classify_texts(list(dataset[NOTE_TEXT_FINAL]), langDetectionModel, langDetectionTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2eb19-511b-4430-b089-8d2cf136f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tweet languages\n",
    "tweetLangs, tweetConfidence = classify_texts(list(dataset[TWEET_TEXT_FINAL]), langDetectionModel, langDetectionTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac432b-1821-4a95-8656-a0856d81ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment dataset\n",
    "dataset[NOTE_LANG] = noteLangs\n",
    "dataset[NOTE_LANG_CONFIDENCE] = noteConfidence\n",
    "dataset[TWEET_LANG] = tweetLangs\n",
    "dataset[TWEET_LANG_CONFIDENCE] = tweetConfidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26751086-0c0e-4569-ab95-cdd4e4ddf48e",
   "metadata": {},
   "source": [
    "## Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f4995-cfac-44df-8a85-b83438fa8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile note lang confidence\n",
    "dataset[NOTE_LANG_CONFIDENCE].plot.hist(bins=50, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415bab34-0edf-4c89-8931-bea7dd880bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile tweet lang confidence\n",
    "dataset[TWEET_LANG_CONFIDENCE].plot.hist(bins=50, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ae15a-21ca-428e-b6c8-d0d3fde4475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile note lang\n",
    "dataset[NOTE_LANG].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94542a0-6e54-485c-bc5e-27e22af60d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile tweet lang\n",
    "dataset[TWEET_LANG].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d413c-7e07-4448-9f05-b28a286e9758",
   "metadata": {},
   "source": [
    "## Set Inferred Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd289b2-4a50-49ea-afa3-14b60bb00e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helpers to infer note langauge and tweet language\n",
    "def infer_note_lang(noteLang, noteConfidence, tweetLang, tweetConfidence):\n",
    "  if noteConfidence > .5:\n",
    "    return noteLang\n",
    "  elif tweetConfidence > .5:\n",
    "    return tweetLang\n",
    "  else:\n",
    "    return pd.NA\n",
    "\n",
    "def infer_tweet_lang(noteLang, noteConfidence, tweetLang, tweetConfidence):\n",
    "  if tweetConfidence > .5:\n",
    "    return tweetLang\n",
    "  elif noteConfidence > .5:\n",
    "    return noteLang\n",
    "  else:\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c5c46-1cbe-4f8c-89c5-173b900337e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inference\n",
    "dataset[NOTE_LANG_INFERRED] = [\n",
    "  infer_note_lang(noteLang, noteConfidence, tweetLang, tweetConfidence)\n",
    "  for (noteLang, noteConfidence, tweetLang, tweetConfidence)\n",
    "  in dataset[[NOTE_LANG, NOTE_LANG_CONFIDENCE, TWEET_LANG, TWEET_LANG_CONFIDENCE]].values\n",
    "]\n",
    "dataset[TWEET_LANG_INFERRED] = [\n",
    "  infer_tweet_lang(noteLang, noteConfidence, tweetLang, tweetConfidence)\n",
    "  for (noteLang, noteConfidence, tweetLang, tweetConfidence)\n",
    "  in dataset[[NOTE_LANG, NOTE_LANG_CONFIDENCE, TWEET_LANG, TWEET_LANG_CONFIDENCE]].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d472826-0255-475b-8755-ad398f2af223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile note lang\n",
    "dataset[NOTE_LANG_INFERRED].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5baae07-0f50-44e0-86b8-8cc1377d5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile note lang\n",
    "dataset[TWEET_LANG_INFERRED].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f020ce88-a6d1-4227-8e9c-9e09b8129aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset with augmented text, labeling signals and language\n",
    "dataset.to_parquet(os.path.join(DATA_ROOT, \"augmented_posts_with_signals_and_langs.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839476e-307f-474b-a5f7-293d2bc979e4",
   "metadata": {},
   "source": [
    "# Prepare Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba43a1-6867-43ae-9f7d-c6d389ce1fc7",
   "metadata": {},
   "source": [
    "## Prune By Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909de2f-c2f3-49af-83e6-6b6e9f0924ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to EN notes and posts\n",
    "print(len(dataset))\n",
    "enDataset = dataset[\n",
    "  (dataset[NOTE_LANG_INFERRED] == \"en\")\n",
    "  & (dataset[TWEET_LANG_INFERRED] == \"en\")\n",
    "]\n",
    "print(len(enDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef77dd-df95-4721-af40-8a6e68781ab4",
   "metadata": {},
   "source": [
    "## Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473c478-5810-4a2b-99eb-e9fc0623d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to apply thresholds and generate multitask labels\n",
    "def make_multitask_dataset(\n",
    "  dataset,\n",
    "  # Tag thresholds\n",
    "  minTotalSignal=3,\n",
    "  minPosRatio=.25,\n",
    "  minPosSignal=2.5,\n",
    "  maxNegRatio=0.1,\n",
    "  # CRNH and Large Factor thresholds\n",
    "  interceptThreshold=0,\n",
    "  largeFactorPosThreshold=0.6,\n",
    "  largeFactorNegThreshold=0.4,\n",
    "):\n",
    "  print(f\"Initial dataset length: {len(dataset)}\")\n",
    "  dataset = dataset[dataset[CLASSIFICATION].notna()]\n",
    "  print(f\"Dataset with classification: {len(dataset)}\")\n",
    "  output = dataset[[NOTE_ID, TWEET_ID, NOTE_TEXT_FINAL, TWEET_TEXT_FINAL]].copy()\n",
    "  # Set label for each tag column\n",
    "  for col in TAG_LABEL_COLS:\n",
    "    posRows = (\n",
    "      (dataset[TOTAL_SIGNAL] >= minTotalSignal)\n",
    "      & (dataset[f\"{col}_ratio\"] >= minPosRatio)\n",
    "      & (dataset[col] >= minPosSignal)\n",
    "      & (dataset[CLASSIFICATION] == MISINFORMED_OR_POTENTIALLY_MISLEADING)\n",
    "    ).astype(np.bool).values\n",
    "    negRows = (\n",
    "      (dataset[TOTAL_SIGNAL] >= minTotalSignal)\n",
    "      & (dataset[f\"{col}_ratio\"] <= maxNegRatio)\n",
    "      & (dataset[CLASSIFICATION] == MISINFORMED_OR_POTENTIALLY_MISLEADING)\n",
    "    ).astype(np.bool).values\n",
    "    assert (posRows & negRows).sum() == 0\n",
    "    output[col] = np.nan\n",
    "    output.loc[posRows, col] = 1.0\n",
    "    output.loc[negRows, col] = 0.0\n",
    "  # Prepare CRNH labels\n",
    "  posRows = (\n",
    "    (dataset[CURRENT_LABEL] == CURRENTLY_RATED_NOT_HELPFUL)\n",
    "    & (dataset[INTERCEPT] < interceptThreshold)\n",
    "  )\n",
    "  negRows = (\n",
    "    (dataset[CURRENT_LABEL] != CURRENTLY_RATED_NOT_HELPFUL)\n",
    "    & (dataset[INTERCEPT] > interceptThreshold)\n",
    "  )\n",
    "  assert (posRows & negRows).sum() == 0\n",
    "  output[CRNH] = np.nan\n",
    "  output.loc[posRows, CRNH] = 1.0\n",
    "  output.loc[negRows, CRNH] = 0.0\n",
    "  # Prepare large factor labels\n",
    "  posRows = dataset[FACTOR].abs() > largeFactorPosThreshold\n",
    "  negRows = dataset[FACTOR].abs() < largeFactorNegThreshold\n",
    "  assert (posRows & negRows).sum() == 0\n",
    "  output[LARGE_FACTOR] = np.nan\n",
    "  output.loc[posRows, LARGE_FACTOR] = 1.0\n",
    "  output.loc[negRows, LARGE_FACTOR] = 0.0\n",
    "  # Prepare classification labels\n",
    "  posRows = dataset[CLASSIFICATION] == MISINFORMED_OR_POTENTIALLY_MISLEADING\n",
    "  negRows = dataset[CLASSIFICATION] == NOT_MISLEADING\n",
    "  assert (posRows & negRows).sum() == 0\n",
    "  assert (posRows | negRows).sum() == len(dataset)\n",
    "  output[CLASSIFICATION] = np.nan\n",
    "  output.loc[posRows, CLASSIFICATION] = 1.0\n",
    "  output.loc[negRows, CLASSIFICATION] = 0.0\n",
    "  # Prepare relevance labels\n",
    "  output[RELEVANCE] = 1.0\n",
    "  output.loc[dataset[CLASSIFICATION] == NOT_MISLEADING, RELEVANCE] = np.nan\n",
    "  output.loc[dataset[INTERCEPT] < interceptThreshold, RELEVANCE] = np.nan\n",
    "  output.loc[dataset[CURRENT_LABEL] == CURRENTLY_RATED_NOT_HELPFUL, RELEVANCE] = np.nan\n",
    "  print(f\"Final dataset length: {len(output)}\")\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52382be-c191-4c9a-89ae-54e5c91b81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present labeled dataset\n",
    "labeledDataset = make_multitask_dataset(enDataset)\n",
    "labeledDataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddb911-d702-448c-b95b-74165ba9e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize labels\n",
    "def count_values_per_column(df):\n",
    "  # Iterate through each column\n",
    "  nanCount = []\n",
    "  zeroCount = []\n",
    "  oneCount = []\n",
    "  for col in df.columns:\n",
    "    # Count values using value_counts, filling missing values with 0\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    zeroCount.append(counts.get(0, 0))\n",
    "    oneCount.append(counts.get(1, 0))\n",
    "    nanCount.append(counts.get(np.nan, 0))\n",
    "  # Create result DataFrame\n",
    "  result = pd.DataFrame({\n",
    "    \"columnName\": df.columns,\n",
    "    \"zeroCount\": zeroCount,\n",
    "    \"oneCount\": oneCount,\n",
    "    \"nanCount\": nanCount,\n",
    "  })  \n",
    "  return result[[\"columnName\", \"nanCount\", \"zeroCount\", \"oneCount\"]]\n",
    "\n",
    "count_values_per_column(labeledDataset[LABEL_COLS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d43210-d02e-46bd-979e-610c00778935",
   "metadata": {},
   "source": [
    "## Generate Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25a777-315c-4399-9984-3e9b4c3c9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for training and testing\n",
    "def split_dataset(dataset, trainFrac=.9):\n",
    "  trainTweets = dataset[TWEET_ID].drop_duplicates().sample(frac=trainFrac)\n",
    "  return (\n",
    "    dataset[dataset[TWEET_ID].isin(trainTweets)],\n",
    "    dataset[~dataset[TWEET_ID].isin(trainTweets)],\n",
    "  )\n",
    "\n",
    "trainSplit, testSplit = split_dataset(labeledDataset)\n",
    "print(len(trainSplit))\n",
    "print(len(testSplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a03e2-a32e-41fd-9640-9deb50708394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to add synthetic relevance examples\n",
    "def add_relevance(dataset, negFactor=5, seed=42):\n",
    "  noteNegRows = pd.concat([dataset[[NOTE_ID, NOTE_TEXT_FINAL]]] * negFactor).reset_index(drop=True)\n",
    "  tweetNegRows = pd.concat([dataset[[TWEET_ID, TWEET_TEXT_FINAL]]] * negFactor).sample(frac=1., random_state=seed).reset_index(drop=True)\n",
    "  relevanceNegRows = pd.concat([noteNegRows, tweetNegRows], axis=1)\n",
    "  relevanceNegRows[LABEL_COLS] = np.nan\n",
    "  relevanceNegRows[RELEVANCE] = 0.0\n",
    "  return pd.concat([dataset, relevanceNegRows], axis=0).sample(frac=1., random_state=seed)\n",
    "\n",
    "trainDataset = add_relevance(trainSplit)\n",
    "print(len(trainDataset))\n",
    "testDataset = add_relevance(testSplit)\n",
    "print(len(testDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8aeafd-1ceb-4f4d-9798-1e3521c89375",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_values_per_column(trainDataset[LABEL_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590be3d6-5053-4c86-8c62-122cfc310883",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_values_per_column(testDataset[LABEL_COLS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d3ec2-6f94-4536-afb1-73d10fb0afa4",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa64162-3ce1-4a0f-ae3e-87c54a1b486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to extract embeddings\n",
    "def make_tensors(dataset, batchSize=1024):\n",
    "  # Prepare tokenizer and inputs\n",
    "  tokenizer = AutoTokenizer.from_pretrained(os.path.join(MODEL_ROOT, DISTILROBERTA_BASE_MODEL, TOKENIZER_DIR))\n",
    "  noteTexts = list(dataset[NOTE_TEXT_FINAL].values)\n",
    "  tweetTexts = list(dataset[TWEET_TEXT_FINAL].values)\n",
    "  assert len(noteTexts) == len(tweetTexts)\n",
    "  # Tokenize all texts\n",
    "  numBatches = int(np.ceil(len(noteTexts) / batchSize))\n",
    "  progressBar = tqdm(range(numBatches))\n",
    "  inputIds = []\n",
    "  attentionMasks = []\n",
    "  start = 0\n",
    "  while start < len(noteTexts):\n",
    "    end = start + batchSize\n",
    "    batch = tokenizer(\n",
    "      list(zip(tweetTexts[start:end], noteTexts[start:end])),\n",
    "      max_length=512,\n",
    "      truncation=\"longest_first\",\n",
    "      padding=\"max_length\",  # Pad to max length since batches are large enough we effectively do this anyways.\n",
    "      return_tensors=\"pt\"\n",
    "    )\n",
    "    inputIds.append(batch[\"input_ids\"])\n",
    "    attentionMasks.append(batch[\"attention_mask\"])\n",
    "    start = end\n",
    "    progressBar.update(1)\n",
    "  # Generate labels and loss mask\n",
    "  labels = torch.tensor(dataset[LABEL_COLS].fillna(0.5).values).to(torch.float32)\n",
    "  lossMask = torch.tensor(dataset[LABEL_COLS].notna().values).to(torch.float32)\n",
    "  return (\n",
    "    torch.concat(inputIds, axis=0),\n",
    "    torch.concat(attentionMasks, axis=0),\n",
    "    labels,\n",
    "    lossMask,\n",
    "    torch.tensor(dataset[NOTE_ID].values),\n",
    "    torch.tensor(dataset[TWEET_ID].values),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76347a10-d6a5-4ba3-ae63-9c1fe8626c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain tokens and masks\n",
    "trainTensors = make_tensors(trainDataset)\n",
    "testTensors = make_tensors(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c2a04-baa2-412b-8a47-1c0f5d639ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors to disk\n",
    "def save_tensors(tensors, fileName):\n",
    "  inputIds, attentionMasks, labels, lossMask, noteIds, tweetIds = tensors\n",
    "  path = os.path.join(DATA_ROOT, fileName)\n",
    "  print(f\"Saving checkpoint to {path}\")\n",
    "  torch.save({\n",
    "    \"inputIds\": inputIds,\n",
    "    \"attentionMasks\": attentionMasks,\n",
    "    \"labels\": labels,\n",
    "    \"lossMask\": lossMask,\n",
    "    \"noteIds\": noteIds,\n",
    "    \"tweetIds\": tweetIds,    \n",
    "  }, path)\n",
    "\n",
    "save_tensors(trainTensors, \"train_tensors.pt\")\n",
    "save_tensors(testTensors, \"test_tensors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f11a63-18d6-4857-9959-bbbbd5f01eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate small splits for testing\n",
    "trainTensorsSmall = tuple(tmp[:2000] for tmp in trainTensors)\n",
    "testTensorsSmall = tuple(tmp[:2000] for tmp in testTensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8037f4a-4795-4e95-91a3-30b216ce42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate small splits for testing\n",
    "trainTensorsMedium = tuple(tmp[:20000] for tmp in trainTensors)\n",
    "testTensorsMedium = tuple(tmp[:20000] for tmp in testTensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3392b-8073-4211-932a-17ebc01aa217",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2602c-73df-4cbc-bc66-81b6736886c7",
   "metadata": {},
   "source": [
    "## Define Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52104be-2161-4ac7-9dfc-1a4084cb7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class ParallelStack(nn.Module):\n",
    "\n",
    "  def __init__(self, hFactor, nHeads, dim=768, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.preclassifier = nn.Linear(dim, hFactor * dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.classifier = nn.Linear(hFactor * dim, nHeads)\n",
    "\n",
    "  def forward(self, embedding):\n",
    "    z = self.preclassifier(embedding)\n",
    "    a = self.dropout(nn.ReLU()(z))\n",
    "    return self.classifier(a)\n",
    "\n",
    "class MultiHeadMLP(nn.Module):\n",
    "\n",
    "  def __init__(self, hFactor=12, dim=768, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.roberta = AutoModel.from_pretrained(os.path.join(MODEL_ROOT, DISTILROBERTA_BASE_MODEL, MODEL_DIR))\n",
    "    self.relevanceClassifier = ParallelStack(hFactor, 1)\n",
    "    self.tagClassifier = ParallelStack(hFactor, len(LABEL_COLS) - 1)\n",
    "\n",
    "  def forward(self, inputIds, attentionMask):\n",
    "    embedding = self.roberta(\n",
    "      input_ids=inputIds,\n",
    "      attention_mask=attentionMask,\n",
    "    ).last_hidden_state[:, 0]  # batch, token, dimension\n",
    "    return torch.concat([\n",
    "      self.relevanceClassifier(embedding),\n",
    "      self.tagClassifier(embedding)\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee87832-8957-489c-8192-343717da0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to prepare loss weights\n",
    "def make_loss_weights(lossMask, objectiveWeights, numBatches):\n",
    "  # Compute weight to assign to each instance of training data for a particular objective\n",
    "  assert objectiveWeights.shape[0] == lossMask.shape[1]\n",
    "  instanceWeights = lossMask.sum(axis=0) ** -1\n",
    "  assert objectiveWeights.shape[0] == instanceWeights.shape[0]\n",
    "  assert np.abs(objectiveWeights.sum().item() - 1) < 1e-5\n",
    "  adjustedInstanceWeight = instanceWeights * objectiveWeights\n",
    "  # Compute weight to assign to each prediction loss\n",
    "  predictionLoss = lossMask * adjustedInstanceWeight\n",
    "  assert predictionLoss.shape == lossMask.shape\n",
    "  assert np.abs(predictionLoss.sum().item() - 1) < 1e-5\n",
    "  assert ((predictionLoss.sum(axis=0) - objectiveWeights).abs() < 1e-5).all().item()\n",
    "  # Apply batch scaling\n",
    "  return numBatches * predictionLoss \n",
    "\n",
    "make_loss_weights(\n",
    "  torch.tensor([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "  ], dtype=torch.float32),\n",
    "  torch.tensor([0.2, 0.2, 0.5, .1], dtype=torch.float32),\n",
    "  10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91279d63-053b-4647-858f-b4c3f810428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to compute loss\n",
    "def multihead_loss(logits, lossWeights, labels):\n",
    "  # Validate sizes match\n",
    "  assert logits.shape == lossWeights.shape\n",
    "  assert logits.shape == labels.shape\n",
    "  # Compute loss of each prediction\n",
    "  return (nn.BCEWithLogitsLoss(reduction=\"none\")(logits, labels) * lossWeights)\n",
    "\n",
    "multihead_loss(\n",
    "  torch.arange(-3, 5, dtype=torch.float32).reshape(2, 4),\n",
    "  torch.tensor([1, 0, 1, 0, 0, 1, 0, 1], dtype=torch.float32).reshape(2, 4),\n",
    "  torch.ones(8, dtype=torch.float32).reshape(2, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad679d-55b5-4568-988b-2042f05899ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper for applying model\n",
    "def apply_model(model, dataset, device, gpuBatchSize, frac=None):\n",
    "  # Configure batching\n",
    "  if device == CUDA and torch.cuda.device_count() > 1:\n",
    "    batchSize = gpuBatchSize * torch.cuda.device_count()\n",
    "  else:\n",
    "    batchSize = gpuBatchSize\n",
    "  # Prepare data\n",
    "  inputIds, attentionMask, labels, lossMask, noteIds, tweetIds = dataset\n",
    "  assert inputIds.shape[0] == attentionMask.shape[0] == labels.shape[0] == lossMask.shape[0] == noteIds.shape[0] == tweetIds.shape[0]\n",
    "  if frac != None:\n",
    "    assert 0 < frac <= 1.\n",
    "    size = int(frac * inputIds.shape[0])\n",
    "    indices = torch.randperm(inputIds.shape[0])[:size]\n",
    "    inputIds = inputIds[indices]\n",
    "    attentionMask = attentionMask[indices]\n",
    "    labels = labels[indices]\n",
    "    lossMask = lossMask[indices]\n",
    "    noteIds = noteIds[indices]\n",
    "    tweetIds = tweetIds[indices]\n",
    "  # Process chunks\n",
    "  start = 0\n",
    "  assert not model.training\n",
    "  preds = []\n",
    "  progress = tqdm(range(int(np.ceil(inputIds.shape[0] / batchSize))))\n",
    "  while start < inputIds.shape[0]:\n",
    "    end = start + batchSize\n",
    "    with torch.no_grad():\n",
    "      with autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        preds.append(\n",
    "          model(\n",
    "            inputIds[start:end].to(device),\n",
    "            attentionMask[start:end].to(device),\n",
    "          ).to(CPU).detach()\n",
    "        )\n",
    "    start = end\n",
    "    progress.update(1)\n",
    "  preds = torch.concat(preds, axis=0)\n",
    "  return labels, preds, lossMask, noteIds, tweetIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e015573-af21-4dfc-a97f-6bd082de41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper for incremental eval\n",
    "def eval_model(model, trainDataset, testDataset, device, gpuBatchSize):\n",
    "  print(\"Forward pass on training data:\")\n",
    "  allTrainLabels, allTrainPreds, allTrainMask, _, _ = apply_model(model, trainDataset, device, gpuBatchSize=gpuBatchSize, frac=(1/9))\n",
    "  print(\"Forward pass on test data:\")\n",
    "  allTestLabels, allTestPreds, allTestMask, _, _ = apply_model(model, testDataset, device, gpuBatchSize=gpuBatchSize, frac=None)\n",
    "  results = []\n",
    "  for i in range(allTrainLabels.shape[1]):\n",
    "    trainLabels, trainPreds = allTrainLabels[allTrainMask[:, i] == 1, i], allTrainPreds[allTrainMask[:, i] == 1, i]\n",
    "    testLabels, testPreds = allTestLabels[allTestMask[:, i] == 1, i], allTestPreds[allTestMask[:, i] == 1, i]\n",
    "    if trainLabels.sum().item() == 0 or testLabels.sum().item() == 0:\n",
    "      results.append((-1, -1, -1, -1))\n",
    "      continue\n",
    "    trainAuc = skm.roc_auc_score(trainLabels.numpy(), trainPreds.numpy())\n",
    "    testAuc = skm.roc_auc_score(testLabels.numpy(), testPreds.numpy())\n",
    "    fpr, tpr, _ = skm.roc_curve(testLabels.numpy(), testPreds.numpy())\n",
    "    tprAt1 = tpr[np.argmin(np.abs(fpr - .01))]\n",
    "    tprAt5 = tpr[np.argmin(np.abs(fpr - .05))]\n",
    "    results.append((trainAuc, testAuc, tprAt1, tprAt5))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6556b6-a14b-4054-a30d-4e3e30a7e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helpers to save checkpointed state\n",
    "def save_checkpoint(root, epoch, batch, loss, model, optimizer, scheduler, scaler, stats):\n",
    "  checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"batch\": batch,\n",
    "    \"loss\": loss,\n",
    "    \"model\": model.module.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n",
    "    \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
    "    \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
    "    \"stats\": stats,\n",
    "  }\n",
    "  checkpointId = str(int(time.time()))\n",
    "  path = os.path.join(root, f\"{checkpointId}.pt\")\n",
    "  print(f\"Saving checkpoint to {path}\")\n",
    "  torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34dc1a-28c9-4a78-827d-f5f1fcdddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train_model(\n",
    "  model,\n",
    "  trainDataset,\n",
    "  testDataset,\n",
    "  numEpochs=3,\n",
    "  device=CUDA,\n",
    "  deepLogEvery=1,\n",
    "  gpuBatchSize=32,\n",
    "  learningRate=1e-5,\n",
    "  logEvery=None,\n",
    "  robertaWeightDecay=0.01,\n",
    "  relevanceWeightDecay=0.05,\n",
    "  tagWeightDecay=0.25,\n",
    "  learningSchedule=True,\n",
    "  objectiveWeights=None,\n",
    "):\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  # Set up checkpoint directory\n",
    "  modelId = str(int(time.time()))\n",
    "  print(f\"Beginning training run for {modelId}\")\n",
    "  modelRoot = os.path.join(DATA_ROOT, modelId)\n",
    "  os.mkdir(modelRoot)\n",
    "  # Prepare data and batching\n",
    "  print(\"Setting up training\")\n",
    "  model = model.to(device)\n",
    "  if device == CUDA and torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    batchSize = gpuBatchSize * torch.cuda.device_count()\n",
    "  else:\n",
    "    batchSize = gpuBatchSize\n",
    "  inputIds, attentionMask, labels, lossMask, _, _ = trainDataset\n",
    "  numBatches = int(np.ceil(inputIds.shape[0] / batchSize))\n",
    "  progress = tqdm(range(numBatches * numEpochs))\n",
    "  if deepLogEvery is None:\n",
    "    deepLogEvery = max(1, int(np.ceil(numEpochs / 10)))\n",
    "  print(f\"Training data contains {inputIds.shape[0]} rows to be split into {numBatches} batches\")\n",
    "  # Prepare models and data\n",
    "  assert all([\n",
    "    (n.startswith(\"module.roberta\") or n.startswith(\"module.relevanceClassifier\") or n.startswith(\"module.tagClassifier\"))\n",
    "    for n, _ in model.named_parameters()\n",
    "  ])\n",
    "  robertaParams = [p for n, p in model.named_parameters() if n.startswith(\"module.roberta\")]\n",
    "  relevanceParams = [p for n, p in model.named_parameters() if n.startswith(\"module.relevanceClassifier\")]\n",
    "  tagParams = [p for n, p in model.named_parameters() if n.startswith(\"module.tagClassifier\")]\n",
    "  print(\"Parameter groups:\", len(robertaParams), len(relevanceParams), len(tagParams))\n",
    "  optim = torch.optim.AdamW([\n",
    "    {\"params\": robertaParams, \"weight_decay\": robertaWeightDecay},\n",
    "    {\"params\": relevanceParams, \"weight_decay\": relevanceWeightDecay},\n",
    "    {\"params\": tagParams, \"weight_decay\": tagWeightDecay},\n",
    "  ], lr=learningRate)\n",
    "  if learningSchedule:\n",
    "    scheduler = LinearLR(\n",
    "      optim,\n",
    "      start_factor=1.0,  # Start at the initial learning rate\n",
    "      end_factor=0.0,    # End at 0\n",
    "      total_iters=(numBatches * numEpochs),  # Total number of training steps      \n",
    "    )\n",
    "  scaler = GradScaler()\n",
    "  if objectiveWeights is None:\n",
    "    objectiveWeights = torch.ones(labels.shape[1]) / labels.shape[1]\n",
    "  assert np.abs(objectiveWeights.sum().item() - 1) < 1e-5\n",
    "  lossWeights = make_loss_weights(lossMask, objectiveWeights, numBatches).to(device)\n",
    "  model.train()\n",
    "  for epoch in range(numEpochs):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    losses = []\n",
    "    base = 0\n",
    "    randOrder = np.random.permutation(np.arange(0, inputIds.shape[0]))\n",
    "    inputIds = inputIds[randOrder]\n",
    "    attentionMask = attentionMask[randOrder]\n",
    "    labels = labels[randOrder]\n",
    "    lossWeights = lossWeights[randOrder]\n",
    "    for batch in range(numBatches):\n",
    "      # Obtain batch\n",
    "      start = batch * batchSize\n",
    "      end = start + batchSize\n",
    "      y = labels[start:end].to(device)\n",
    "      with autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        # Forward pass\n",
    "        y_hat = model(\n",
    "          inputIds[start:end].to(device),\n",
    "          attentionMask[start:end].to(device),\n",
    "        )\n",
    "        # Compute loss\n",
    "        loss = multihead_loss(y_hat, lossWeights[start:end], y).sum()\n",
    "      losses.append(loss.item())\n",
    "      # Backward pass\n",
    "      scaler.scale(loss).backward()\n",
    "      # Update weights\n",
    "      scaler.step(optim)\n",
    "      scaler.update()\n",
    "      if learningSchedule:\n",
    "        scheduler.step()\n",
    "      # Zero out gradients\n",
    "      optim.zero_grad()\n",
    "      # Update progress bar\n",
    "      progress.update(1)\n",
    "      if batch % logEvery == 0:\n",
    "        print(f\"epoch={epoch:<3d}  batch={batch:<5d}  loss={np.mean(losses[-logEvery:]):7.5f}\")\n",
    "    # Log loss\n",
    "    model.eval()\n",
    "    results = eval_model(model, trainDataset, testDataset, device, gpuBatchSize=gpuBatchSize)\n",
    "    stats = []\n",
    "    for (trainAuc, testAuc, tprAt1, tprAt5), label in zip(results, LABEL_COLS):\n",
    "      print(f\"  epoch={epoch:<3d}  loss={np.mean(losses):7.5f}  trainAuc={trainAuc:5.3f}  testAuc={testAuc:5.3f}  tpr@0.01={tprAt1:5.3f}  tpr@0.05={tprAt5:5.3f}  ({label})\")\n",
    "      stats.append((label, trainAuc, testAuc, tprAt1, tprAt5))\n",
    "    save_checkpoint(modelRoot, epoch, batch, np.mean(losses), model, optim, scheduler, scaler, stats)      \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcf358-735c-44aa-ae1e-fefe07f419b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hparams(\n",
    "  trainDataset,\n",
    "  testDataset,\n",
    "  numEpochs=5,\n",
    "  gpuBatchSize=32,\n",
    "  learningRate=1e-5,\n",
    "  learningSchedule=True,\n",
    "  robertaWeightDecay=0.01,\n",
    "  relevanceWeightDecay=0.05,\n",
    "  tagWeightDecay=0.25,\n",
    "  objectiveWeights=None,\n",
    "  logEvery=500,\n",
    "):\n",
    "  model = MultiHeadMLP()\n",
    "  train_model(\n",
    "    model,\n",
    "    trainDataset,\n",
    "    testDataset, \n",
    "    numEpochs=numEpochs,\n",
    "    gpuBatchSize=gpuBatchSize,\n",
    "    learningRate=learningRate,\n",
    "    learningSchedule=learningSchedule,\n",
    "    robertaWeightDecay=robertaWeightDecay,\n",
    "    relevanceWeightDecay=robertaWeightDecay,\n",
    "    tagWeightDecay=robertaWeightDecay,\n",
    "    objectiveWeights=objectiveWeights,\n",
    "    logEvery=logEvery,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e98516-ee6b-4111-9db8-376d480e9041",
   "metadata": {},
   "source": [
    "## Small Scale Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e86aa-1572-4f20-9cad-c0ec662d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run at small scale\n",
    "eval_hparams(\n",
    "  trainTensorsSmall,\n",
    "  testTensorsSmall,\n",
    "  numEpochs=1,\n",
    "  logEvery=1,\n",
    "  gpuBatchSize=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344925e-b4f9-4619-910b-789115455faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run at medium scale\n",
    "eval_hparams(\n",
    "  trainTensorsMedium,\n",
    "  testTensorsMedium,\n",
    "  numEpochs=3,\n",
    "  logEvery=10,\n",
    "  gpuBatchSize=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f832c-d724-410b-8147-093682da853d",
   "metadata": {},
   "source": [
    "## Full Scale Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe9c08-d8b1-49e9-b1f1-e69341e59c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train at full scale\n",
    "eval_hparams(\n",
    "  trainTensors,\n",
    "  testTensors,\n",
    "  numEpochs=3,\n",
    "  logEvery=500,\n",
    "  gpuBatchSize=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1305d42-f06a-4136-9bc0-be365be7ae91",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d2cfa-3405-467b-acd1-396638b625b8",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52eac1d-8a47-49d4-af65-6196b20f96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def load_checkpoint(path):\n",
    "  # Log checkpoint state\n",
    "  print(f\"Loading checkpoint from {path}\")\n",
    "  checkpoint = torch.load(path, weights_only=False)\n",
    "  print(f\"  epoch={checkpoint['epoch']:<3d}\")\n",
    "  print(f\"  batch={checkpoint['batch']:<3d}\")\n",
    "  print(f\"  loss={checkpoint['loss']:7.5f}\")\n",
    "  for label, trainAuc, testAuc, tprAt1, tprAt5 in checkpoint[\"stats\"]:\n",
    "    print(f\"  {label:<40} trainAuc={trainAuc:5.3f}  testAuc={testAuc:5.3f}  tpr@0.01={tprAt1:5.3f}  tpr@0.05={tprAt5:5.3f}\")\n",
    "  # load model\n",
    "  model = MultiHeadMLP()\n",
    "  model.load_state_dict(checkpoint[\"model\"])\n",
    "  return model\n",
    "\n",
    "evaluator = load_checkpoint(\"/path/to/chosen/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b81256-2471-42f8-b476-2d1776ac0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model\n",
    "def verify_model(model, trainTensors, testTensors):\n",
    "  model = nn.DataParallel(model.to(CUDA))\n",
    "  model.eval()\n",
    "  stats = eval_model(model, trainTensors, testTensors, CUDA, 32)\n",
    "  for label, (trainAuc, testAuc, tprAt1, tprAt5) in zip(LABEL_COLS, stats):\n",
    "    print(f\"  {label:<40} trainAuc={trainAuc:5.3f}  testAuc={testAuc:5.3f}  tpr@0.01={tprAt1:5.3f}  tpr@0.05={tprAt5:5.3f}\")  \n",
    "\n",
    "verify_model(evaluator, trainTensors, testTensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae21be6-9d38-4560-a517-4f8756ca6054",
   "metadata": {},
   "source": [
    "## Generate Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e99308-4688-48b3-b273-e068d79c496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply model to see sample results\n",
    "evaluator.eval()\n",
    "evaluator.to(CUDA)(testTensors[0][:3].to(CUDA), testTensors[1][:3].to(CUDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b18f5f-6a71-43e0-996f-1a1301ad3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper for applying model\n",
    "def get_jit_trace(model, tensors):\n",
    "  # Prepare data\n",
    "  model = model.to(CPU)\n",
    "  inputIds, attentionMask, labels, lossMask, noteIds, tweetIds = tensors\n",
    "  assert inputIds.shape[0] == attentionMask.shape[0] == labels.shape[0] == lossMask.shape[0] == noteIds.shape[0] == tweetIds.shape[0]\n",
    "  assert not model.training\n",
    "  with torch.no_grad():\n",
    "    return torch.jit.trace(model, (inputIds[:1], attentionMask[:1]))\n",
    "\n",
    "torch.jit.save(get_jit_trace(evaluator, testTensors), os.path.join(DATA_ROOT, \"model.jit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf159bc-0ecd-4d04-9d6e-4348c987b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Validate model loaded from disk\n",
    "def validate_jit_model(path, testTensors):\n",
    "  model = torch.jit.load(path)\n",
    "  assert not model.training\n",
    "  return model(testTensors[0][:3].to(CPU), testTensors[1][:3].to(CPU))\n",
    "\n",
    "validate_jit_model(os.path.join(DATA_ROOT, \"model.jit\"), testTensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ea270-fc12-469c-a6f0-ce04e408e170",
   "metadata": {},
   "source": [
    "## Pack Tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de75f9f-f1e9-473c-ae31-60db5d02d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to create a tarball from a list of pairs\n",
    "def pack_tarball(data: list[tuple[str, bytes]]) -> bytes:\n",
    "  tarBytes = io.BytesIO()\n",
    "  with tarfile.open(fileobj=tarBytes, mode='w') as tar:\n",
    "    for name, content in data:\n",
    "      print(name)\n",
    "      # Encode content if it's a string\n",
    "      assert isinstance(content, bytes)\n",
    "      # Create TarInfo object\n",
    "      info = tarfile.TarInfo(name=name)\n",
    "      info.size = len(content)\n",
    "      # Add file to tar\n",
    "      tar.addfile(info, io.BytesIO(content))\n",
    "  \n",
    "  # Reset the stream position to the beginning\n",
    "  tarBytes.seek(0)\n",
    "  return tarBytes.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aabe84-45af-40ed-bbc3-89848d9d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to prepare test data\n",
    "def prepare_test_dataset(dataset, tensors, model, size=25):\n",
    "  # Select test samples\n",
    "  inputIds, attentionMask, labels, lossMask, noteIds, tweetIds = tensors\n",
    "  preds = model(inputIds[:size].to(CPU), attentionMask[:size].to(CPU))\n",
    "  result = pd.DataFrame({\n",
    "    NOTE_ID: noteIds[:size].numpy(),\n",
    "    TWEET_ID: tweetIds[:size].numpy(),\n",
    "  })\n",
    "  result[LABEL_COLS] = preds.detach().to(CPU).numpy()\n",
    "  result = result[[NOTE_ID, TWEET_ID, RELEVANCE, \"notHelpfulSpamHarassmentOrAbuse\"]]\n",
    "  # Merge with raw inputs.  Note that any synthetic note/tweet pairs will be dropped\n",
    "  # because they don't occur in the dataset\n",
    "  result = dataset[[NOTE_ID, TWEET_ID, NOTE_TEXT, TWEET_TEXT, TWEET_SHORTEN_URLS, TWEET_EXPANDED_URLS]].merge(result)\n",
    "  assert len(result) > 0\n",
    "  return result\n",
    "\n",
    "prepare_test_dataset(dataset, testTensors, evaluator.to(CPU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60a510-c1c8-4d91-8a66-235b089b5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a tarball containing all modeling resources\n",
    "def create_tarball(testData):\n",
    "  # List of {path, resource} pairs\n",
    "  pairs = []\n",
    "  # Add tokenizer resources\n",
    "  tokenizerDir = os.path.join(MODEL_ROOT, DISTILROBERTA_BASE_MODEL, TOKENIZER_DIR)\n",
    "  for fileName in os.listdir(tokenizerDir):\n",
    "    if fileName.startswith(\".\"):\n",
    "      continue\n",
    "    with open(os.path.join(tokenizerDir, fileName), \"rb\") as handle:\n",
    "      resource = handle.read()\n",
    "    pairs.append((f\"tokenizer/{fileName}\", resource))\n",
    "  # Add jit model\n",
    "  with open(os.path.join(DATA_ROOT, \"model.jit\"), \"rb\") as handle:\n",
    "    jitModel = handle.read()\n",
    "  pairs.append((\"model/model.jit\", jitModel))\n",
    "  # Add labels\n",
    "  labels = b\"\".join(f\"{label}\\n\".encode(\"utf-8\") for label in LABEL_COLS)\n",
    "  pairs.append((\"model/labels.txt\", labels))\n",
    "  # Add test data\n",
    "  buf = io.BytesIO()\n",
    "  testData.to_parquet(buf)\n",
    "  buf.seek(0)\n",
    "  pairs.append((\"test_data.parquet\", buf.getvalue()))\n",
    "  return pack_tarball(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05140a7-5add-4463-b53e-5b8a71dd0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store tarball\n",
    "tarball = create_tarball(prepare_test_dataset(dataset, testTensors, evaluator))\n",
    "print(hashlib.sha256(tarball).hexdigest())\n",
    "with open(os.path.join(DATA_ROOT, \"evaluator.tar\"), \"wb\") as handle:\n",
    "  handle.write(tarball)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
